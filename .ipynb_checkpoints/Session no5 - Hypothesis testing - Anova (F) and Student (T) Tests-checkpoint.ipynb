{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "#To calculate area below the normal distribution and calculate probabilities\n",
    "from numpy import trapz \n",
    "from scipy.integrate import simps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_bar_bar = 4.0\n",
      "SCT = 30.0\n",
      "SC_intra = 6.0\n",
      "SC_inter = 24.0\n"
     ]
    }
   ],
   "source": [
    "# part 1 from this vid: https://www.youtube.com/watch?v=tjolTrwJhjM\n",
    "# putain c bien de la merde ca en fait... bon en gros, on considere le jeu de donnnes suivant:\n",
    "# quand tu mettras au propore tu ecris aussi l'enonce que l'on suit ok?\n",
    "    # on suppose que l'on test l'effet de 3 aliments sur un echantillon issue d'une population de personnes...\n",
    "    # chaque colonne decrit les resultats obtenus par les personnes pour un aliment donne\n",
    "    # avec ces resultats experimentaux, state wether or not those aliments have an effect on people (knowing that alpha = 10%)\n",
    "    \n",
    "# On pose les hypotheses:\n",
    "    # H0: aucun des aliments n'a pas d'impacts sur les populations \n",
    "    # et les differences de moyenne observees ne sont pas assez significatives pour deduire d'un quelquonque effet du aux aliments...\n",
    "\n",
    "    # H1: les aliments ont impact, les 3 moyennes different de facon suffisamment significative de la moyenne de la population pour imputer ces differences aux aliments\n",
    "\n",
    "# Ok, travail preparatoire realise... c'est tipar!!\n",
    "# -------------------------------------------------\n",
    "# Definition du dataset (c'est vraiment un dataset bidon lol, jsuis dsl, je reprend l'ex de la video...)\n",
    "df = pd.DataFrame({'Aliment 1':[3,2,1], 'Aliment 2':[5,3,4], 'Aliment 3':[5,6,7]} , index=np.arange(start=0, stop=3, step=1)) \n",
    "df\n",
    "\n",
    "# Now we can calculate X-bar-bar meaning the mean of ALL the values in the dataframe (among all the columns, not per columns, among all of 'em)\n",
    "def get_x_bar_bar_value(df):\n",
    "    df_nbitems = 0\n",
    "    col_sums = []\n",
    "    sum_all_values_in_df =0\n",
    "    for i in range(len(df.columns)):\n",
    "        col = df.iloc[:, i]\n",
    "        col_sum = col.sum()\n",
    "        df_nbitems = df_nbitems+len(col)\n",
    "        col_sums.append(col_sum)\n",
    "    for i in range(len(col_sums)):\n",
    "        sum_all_values_in_df = col_sums[i]+ sum_all_values_in_df\n",
    "    x_bar_bar_value = sum_all_values_in_df/df_nbitems\n",
    "    return x_bar_bar_value\n",
    "\n",
    "x_bar_bar = get_x_bar_bar_value(df)\n",
    "print(\"x_bar_bar = \" + str(x_bar_bar))\n",
    "\n",
    "# We can also calculate what is called SCT \"somme des carres totaux\" in French \n",
    "# it's basically calculate the 'variance' among all the column in the dataset using x_bar_bar as average\n",
    "# So we'd have:\n",
    "\n",
    "def get_SCT(x_bar_bar, df):\n",
    "    SCT = 0\n",
    "    for i in range(len(df.columns)):\n",
    "        col = df.iloc[:, i]\n",
    "        for i in range(len(col)):\n",
    "            SCT = np.square(col[i]-x_b_b) + SCT\n",
    "    return SCT\n",
    "        \n",
    "SCT = get_SCT(x_bar_bar, df)\n",
    "print(\"SCT = \" + str(SCT))\n",
    "\n",
    "\n",
    "# We can also calculate what is called SC_intraclass \n",
    "# it looks like SCT value calculated above, but instead of using x_bar_bar, you wanna use the average value of each column\n",
    "# So we'd have:\n",
    "\n",
    "def get_SC_intra(df):\n",
    "    SC_intra = 0\n",
    "    for i in range(len(df.columns)):\n",
    "        col = df.iloc[:, i]\n",
    "        col_avg = col.sum()/len(col)\n",
    "        for i in range(len(col)):\n",
    "            SC_intra = np.square(col[i]-col_avg)+SC_intra\n",
    "    return SC_intra \n",
    "\n",
    "SC_intra = get_SC_intra(df)\n",
    "print(\"SC_intra = \" + str(SC_intra))\n",
    "    \n",
    "# We can also calculate what is called SC_interclass \n",
    "# it looks like the 2 previous things seen above, but here, you wanna use the average of each column and x_bar_bar\n",
    "# So we'd have:\n",
    "\n",
    "def get_SC_inter(df, x_bar_bar):\n",
    "    SC_inter = 0\n",
    "    for i in range(len(df.columns)):\n",
    "        col = df.iloc[:, i]\n",
    "        col_avg = col.sum()/len(col)\n",
    "        SC_inter = (np.square(col_avg-x_bar_bar)*len(col))+SC_inter\n",
    "    return SC_inter \n",
    "\n",
    "SC_inter = get_SC_inter(df, x_bar_bar)\n",
    "print(\"SC_inter = \" + str(SC_inter))\n",
    "\n",
    "# Rajoute une petite section de texte dans laquelle tu viens simplement ecrire la relation entre ces 3 petites values...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F value = 12.0\n"
     ]
    }
   ],
   "source": [
    "# variable suit une loi F. Dans l'exemple precedent du test d'hypothese \"one sample test\", la variable aleatoire etudiee suit une loi Z\n",
    "# donc on a calcule le valeur de z associe a sa moyenne pour voir si cette valeur correspondait a une valeur \"extreme\" de la distribution Z\n",
    "# valeur extreme si au dela du seuil alpha...\n",
    "\n",
    "# on fait la memem chose ici: a savoir, on sait que les variables etudiees (qui sont 3 instances de la meme variable en fait...) suivent la meme loi F\n",
    "# on calcul cette valeur de F. Puis en se servant de la table de F on voit si la valeur calculee s'avere etre une valeur extreme de cette f distribution\n",
    "# sachant qu'encore une fois, valeur extreme ssi, au dela de ce qui est fixe par alpha (ici 10%)\n",
    "\n",
    "# la formule donnant la valeur de f est la suivante:\n",
    "# F=((SC_intra)/m-1) / ((SC_inter)/m(n-1))\n",
    "# so we'd have in this example: \n",
    "\n",
    "def get_F_value(SC_intra, SC_inter, df):\n",
    "    F = (SC_inter/(len(df.columns)-1)) / (SC_intra/(len(df.columns)*(len(df)-1)))\n",
    "    return F\n",
    "\n",
    "F = get_F_value(SC_intra, SC_inter, df)\n",
    "print(\"F value = \" + str(F))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TO BE DONE]Now the idea is simply to look up in the F table and using the alpha value guess if F we found is in critical region\n",
    "# [TO BE DONE]Here's how you can plot f : http://pytolearn.csd.auth.gr/d1-hyptest/11/f-distro.html\n",
    "    # it might require from you a lil bit of research lol anyways... then just use the video to explain how to use it...\n",
    "    # last vid around (13:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's something else you can work on after you're done with this: http://pytolearn.csd.auth.gr/d1-hyptest/11/f-distro.html\n",
    "    # its the student blog lol, just do the same things you're doing below...\n",
    "\n",
    "# Here are other example you can work on (from khan academy, veeery interesting...): \n",
    "    # Part 1: https://www.youtube.com/watch?v=tjolTrwJhjM\n",
    "    # Part 2: https://www.youtube.com/watch?v=DMo9yofC5C8\n",
    "    # Part 3: https://www.youtube.com/watch?v=y8nRhsixBPs\n",
    "# Among the things you can do/should do\n",
    ": \n",
    "    # I guess just follow along lol\n",
    "    #--------------------------------------------------------------------------------------------\n",
    "    # Always try your best to not just give numerical values but justications using \"ground rules\"    \n",
    "    # create the small datasets he uses in her examples\n",
    "    # calculate stats on em like variance, mean, and stuff like that like in the tea video\n",
    "    # take notes about stuff he says in the video (based on what she says in the tea video)\n",
    "    # ...\n",
    "    # Again, what I want you to do on it is the same as what you did below for the tea exercise...\n",
    "    \n",
    "\n",
    "# Here's an example you can work on A lot of colours in this example, so I want you to do it lol: https://www.youtube.com/watch?v=-yQb_ZJnFXw\n",
    "# Among the things you can do/should do: \n",
    "    # Always try your best to not just give numerical values but justications using \"ground rules\"    \n",
    "    # create the small datasets he uses in her examples\n",
    "    # calculate stats on em like variance, mean, and stuff like that like in the tea video\n",
    "    # take notes about stuff he says in the video (based on what she says in the tea video)\n",
    "    # ...\n",
    "    # Again, what I want you to do on it is the same as what you did below for the tea exercise...\n",
    "    \n",
    "\n",
    "# Here's an example you can work on (tea exercise...): https://www.youtube.com/watch?v=srDr-4cz1KI&feature=emb_logo\n",
    "# Here's the corresponding course: https://www.statisticshowto.com/probability-and-statistics/hypothesis-testing/anova/    \n",
    "# Among the things you can do/should do: \n",
    "    # Always try your best to not just give numerical values but justications using \"ground rules\"\n",
    "    # create the small datasets she uses in her examples\n",
    "    # calculate stats on em like variance, mean, and stuff like that\n",
    "    # take notes about stuff she says in the video\n",
    "    # ...\n",
    "    \n",
    "# Here's another example you can work on: https://www.youtube.com/watch?v=ITf4vHhyGpc\n",
    "    # What you'll do on this is pretty much the same thing as what you did above...\n",
    "\n",
    "# The last one, also interesting: https://www.youtube.com/watch?v=WUjsSB7E-ko\n",
    "    # Same as above...\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally one thing you can do is go through the course material of guyader once you feel like you're good enough..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On considere l'exemple de la video suivante: https://www.youtube.com/watch?v=KGtGgyqKzGQ\n",
    "    # On mesure le taux d'emission d'un echantillon voitures. pour etre conforme aux nouvelles reglementations, ces voitures doivent emettre moins qu'un seuil fixe a 20ppm\n",
    "    # Au regard des resultats du test auquel on soumet ces voitures, peut on dire que c'est le cas?\n",
    "    # alpha est fixe a 0.01\n",
    "    # D'autre questions que je me suis pose:\n",
    "        # [TO BE DONE]Si avec une taille d'echantillon donne, un niveau de confiance alpha donne il etait possible de fixer sans resultat, le seuil au dessous duquel si l'echantillon produit des resultat inferieurs, on pourrait dire avec une confiance de n% que le test est successful\n",
    "        # [TO BE DONE]Si qqn viens avec des resultats d'echantillons me demander \"C'est bon ca passe??\" quelles quesitons pernitentes je dois me poser afin de valider ou non le test de ce quidam? (bien sur j'imagine que taille de l'echantillon et resultats experimentaux sont necessaires, mais n'ais je pas besoin d'au moins une donnee sur la population?)\n",
    "    \n",
    "\n",
    "# Set the test parameters:\n",
    "# H_0: Non, les voitures ne sont pas conformes. Les resultats obtenus ne sont pas assez situes dans les regions extremes de la distribution des moyenne pour pouvoir ecarter la possibilite qu'il soient simplement du a la variabilite de la mesure/le hasard.\n",
    "    # On aurait alors: mu = 2- ppm\n",
    "# H_1: Oui, les moteurs sont bien conformes. Les resultats, pour cette taille d'echantillon sont suffisamment extremes pour etre certain qu'il ne sont pas du au hasard...\n",
    "    # On a alors: H_1 mu<20ppm   \n",
    "# The sample statistics:\n",
    "    # X_bar = 17,17\n",
    "    # s = 2.98\n",
    "# After first quick analysis, we can say that: \n",
    "    # we'll reject H_0 if P(x_bar == 17,17/H_0 vrai)<= 0.01\n",
    "    # In plain english it'd be: \"Having these sample statistics for a sample of this size with a population that doesn't pass this test is nearly impossible\"\n",
    "    # In other words making une erreur de premiere espece (rejeter H_0 alors qu'elle est vraie est quasi impossible)    \n",
    "# Let's perform this test as usual:\n",
    "    # we'll take a look at the distribution of sample means and try to see which law it follows\n",
    "    # then'll we'll calculate the law statistic (here the statistic t) and see if this statistic is in extreme regions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The standard deviation s: 2.9814426038413018\n",
      "The sample mean x_bar:17.169999999999998\n",
      "The sample size n: 10\n",
      "The statistics T:-3.001649525885985\n"
     ]
    }
   ],
   "source": [
    "# This sample is a small sample (n<30) so the statistic we'll calculate is a T statistic, and it follows a student law\n",
    "# We have t = (x_bar - mu_xbar)/(s/sqrt(n)), we know/can easily calculate the following values:\n",
    "    # x_bar\n",
    "    # mu_xbar which is equal to 20 since we're considering H_0 to be true for now\n",
    "    # and n the sample size of 10\n",
    "    \n",
    "# We set mu = 20 as discussed above...\n",
    "mu = 20\n",
    "# How you create a Series\n",
    "series = pd.Series([15.6, 16.2, 22.5, 20.5, 16.4, 19.4, 16.6, 17.9, 12.7, 13.9] , index=np.arange(start=0, stop=10, step=1)) \n",
    "series.index = [0,1,2,3,4,5,6,7,8,9] #If you wanna modify index values\n",
    "s = series.std()\n",
    "x_bar = series.mean()\n",
    "n = len(series)\n",
    "T = (x_bar - mu)/(s/np.sqrt(n))\n",
    "\n",
    "\n",
    "print(\"The standard deviation s: \" + str(s))\n",
    "print(\"The sample mean x_bar:\" + str(x_bar))\n",
    "print(\"The sample size n: \" + str(n))\n",
    "print(\"The statistics T:\" + str(T))\n",
    "\n",
    "# again as explain above, because of its calculation, this staticstics T should follow a normal law, but, because we're on a small dataset, we'll use a different law, the distribuiton we use is still the normal distribution.\n",
    "# But depending on parameters such as the \"degre de liberte\" (size-1 = 9 here) and... we'll simply use a different table, that'll take in account the small size of sample in probability calculus\n",
    "# Here's how we're going to use this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [TO BE DONE]Like for Anova the idea is simply to look up in the T table and using the alpha value (and the \"degre de liberte\" previously calculated) guess if T we found is in critical region\n",
    "# [TO BE DONE]Here's how you can use the T table in the video https://www.youtube.com/watch?v=KGtGgyqKzGQ (it's around 10:00)\n",
    "# Watch out, you want to formulate your findings in the following way:\n",
    "        # pour un seuil de tolerance alpha = 0.01, on cherche la region pour laquelle la probabilite d'avoir des valeurs en dehors de cette region est de 99%\n",
    "        # in other words, we want to have 1 % of the values in our critical region and check if our T stat is in these 1 %\n",
    "        \n",
    "# You already know that we'll reject this H_0 thinggy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's something else you can work on after you're done with this: http://pytolearn.csd.auth.gr/d1-hyptest/12/ttest-indep.html\n",
    "    # its the student blog lol, just do the same things you're doing below...\n",
    "\n",
    "# Here's an example you can work on (from khan academy francophone): https://www.youtube.com/watch?v=KGtGgyqKzGQ\n",
    "# Here's the corresponding course: https://www.youtube.com/watch?v=f7gfCutYXbU\n",
    "# Among the things you can do/should do: \n",
    "    # Always try your best to not just give numerical values but justications using \"ground rules\"\n",
    "    # create the small datasets he uses in her examples\n",
    "    # calculate stats on em like variance, mean, and stuff like that (like in the tea video from the anova part)\n",
    "    # take notes about stuff he says in the video (based on what she did in the tea video from the anova part)\n",
    "    # ...\n",
    "\n",
    "\n",
    "# Hey, can you take a look at this before doing anything else?\n",
    "    # Interval de confiance (Student): https://www.youtube.com/watch?v=i5DmLO1wVqQ\n",
    "\n",
    "\n",
    "# Here's an example you can work on (tea exercise...): https://www.youtube.com/watch?v=fKZA5waOJ0U&feature=emb_logo\n",
    "# Here's the corresponding course: https://www.statisticshowto.com/probability-and-statistics/t-test/    \n",
    "# Among the things you can do/should do: \n",
    "    # Always try your best to not just give numerical values but justications using \"ground rules\"\n",
    "    # create the small datasets she uses in her examples\n",
    "    # calculate stats on em like variance, mean, and stuff like that\n",
    "    # take notes about stuff she says in the video\n",
    "    # ...\n",
    "    \n",
    "# Here's a final example I want you to work on:\n",
    "    # part 1 (\"interval de confiance\" stuff...): https://www.youtube.com/watch?v=sXV1GOVpGsY \n",
    "    # part 2 (actual hypothesis testing): https://www.youtube.com/watch?v=Sl5g5_w7HU4\n",
    "# Eventhough it's not a t-test, I like this example, it's a 2 sample comparison on which you wont use the T test, but the z test seen previously...\n",
    "# What I want you to understand with this, is that: \n",
    "    # there's no such thing as t, or z, or x,y,z test I think... \n",
    "    # I think it's more about the information you have in the enonce that you'll know what kind of formula you'll use to calculate the probability you're looking for\n",
    "    # And based on the same enonce, you'll know which normal law table to use (\"is it a z table?\", \"is it a t table?\", ...).\n",
    "    # Rephrasing the above: \"And based on the enonce, you'll know which law does the variable follows...\"\n",
    "    # This a conclusion you can even put in the \"Summary of all kind of testing\" notebook... \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do: \n",
    "# Maybe inverser l'ordre de anova and student test.. since anova allows \"more\" than t test... anyways, not super important..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annexes:\n",
    "# [in case you really need a course...]: \n",
    "    # https://www.youtube.com/watch?v=FPqeVhtOXEo \n",
    "# [nice interesting other course...]: \n",
    "    # https://www.youtube.com/watch?v=OypCNBPmGBY \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
